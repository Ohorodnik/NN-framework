while not converge:
    sample mini-batches of size=batch_size
    for each mini-batch:
        forward pass (NN)
        compute cost (loss)
        get gradients (autograd)
        get parameter updates (optimizer)
        apply updates
    end for
end while
return NN

sample_batches(X, Y, batch_size) -> iterator over mini-batches

    X           -- (m, N) m = # of features, N = sample size. Inputs.
    Y           -- (n, N) labels. n = output dimmentions.
    batch_size  -- size of mini-batches to use.

    return      -- iterable over mini-batches. [(X_sample, Y_sample), ...]


    shuffle X and Y
    cut X and Y into batches of size batch_size
    return mini_batches  

forward(NN, X) -> prediction

    NN      -- neural network
    X       -- inputs.

    return  -- activation of last layer. (n, 1)

    activation = X
    for layer in NN
        get layer activation
    return last activation

cost(predictions , Y) -> cost estimate

    predictions -- output of NN. (n, 1)
    Y           -- corect labels. (n, 1)

    return      -- cost estimate over mini-batch. float

    compute empirical expectation of loss on mini_batch
    return cost
